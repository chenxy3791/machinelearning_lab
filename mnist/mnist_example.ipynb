{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('tensorflow version is: ',tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print('numpy version is: ',np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare a dataset\n",
    "\n",
    "加载数据集并划分为训练集以及测试集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:10])\n",
    "#print(x_train[1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(1, figsize=(15,10))\n",
    "plt.subplot(131)\n",
    "plt.imshow(x_train[0,:].reshape(28,28))\n",
    "plt.subplot(132)\n",
    "plt.imshow(x_train[1,:].reshape(28,28))\n",
    "plt.subplot(133)\n",
    "plt.imshow(x_train[2,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Normalization\n",
    "\n",
    "Q2. 只需要对x_train做归一化处理吗？这里做x_train的归一化会反映到dataset中去吗？如何确认？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:].reshape(60000,784).astype('float32') / 255\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size = 1024).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation a simple classification model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(784, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation = tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define loss, metric, optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 What is logits?\n",
    "\n",
    "[Wikipedia]In statistics, the logit (/ˈloʊdʒɪt/ LOH-jit) function or the log-odds is the logarithm of the odds p/(1-p) where p is probability. It is a type of function that creates a map of probability values from [0,1] to (-∞,+∞). It is the inverse of the sigmoidal \"logistic\" function or logistic transform used in mathematics, especially in statistics. In deep learning, the term logits layer is popularly used for the last neuron layer of neural networks used for classification tasks, which produce raw prediction values as real numbers ranging from (-∞,+∞).\n",
    "\n",
    "[Stackexchange]\n",
    "In Math, Logit is a function that maps probabilities ([0, 1]) to R ((-inf, inf)).\n",
    "Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to > 0.5.\n",
    "\n",
    "In ML, it can be the vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function. If the model is solving a multi-class classification problem, logits typically become an input to the softmax function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible class.\n",
    "\n",
    "In binary classification problem, logits also sometimes refer to the element-wise inverse of the sigmoid function.\n",
    "\n",
    "简而言之，在分类神经网络模型中，logits就是全连接层的（未经sigmoid或softmax处理）直接输出。在上面的模型中最后一级是没有激活函数的，也就是说模型的最后一层输出的是logits，因此以下loss函数中要指定from_logits = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic loss function that expect integer targets\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an accuracy metric\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training the model\n",
    "\n",
    "Q: In the following iteration loop, what does 'one step' refers to? Is it related to the above 'dataset.shuffle()', esp, the parameters buffer_size, and batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the batches of the dataset\n",
    "for step,(x,y) in enumerate(dataset):\n",
    "    # Open a GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        # loss for this batch\n",
    "        loss_value = loss(y,logits)\n",
    "        \n",
    "    # Get gradients of weights w.r.t the loss\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    \n",
    "    # Update the weights of our linear layer\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_weights))\n",
    "    \n",
    "    # Update the running accuracy\n",
    "    accuracy.update_state(y, logits)\n",
    "    \n",
    "    # Logging\n",
    "    if (step % 100)==0:\n",
    "        print('step = ',step)\n",
    "        print('Loss from the last step: ',loss_value)\n",
    "        print('Total running accuracy so far: ', float(accuracy.result()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Prediction/Evaluation with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[:].reshape(10000,784).astype('float32') / 255\n",
    "logits_test = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits_test[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.argmax(logits_test[:10], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(15,10))\n",
    "plt.subplot(131)\n",
    "plt.imshow(x_test[0,:].reshape(28,28))\n",
    "plt.subplot(132)\n",
    "plt.imshow(x_test[1,:].reshape(28,28))\n",
    "plt.subplot(133)\n",
    "plt.imshow(x_test[2,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
