{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8e003c",
   "metadata": {},
   "source": [
    "# 概要\n",
    "\n",
    "https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/\n",
    "\n",
    "Loss functions play an important role in any statistical model - they define an objective which the performance of the model is evaluated against and the parameters learned by the model are determined by minimizing a chosen loss function.\n",
    "\n",
    "Loss functions define what a good prediction is and isn’t. In short, choosing the right loss function dictates how well your estimator will be. This article will probe into loss functions, the role they play in validating predictions, and the various loss functions used.\n",
    "\n",
    "Prerequisities\n",
    "The reader is expected to have a faint idea of machine learning concepts such as regression and classification, and the basic building blocks that formulate a statistical model that can churn out predictions. Machine Learning Mastery has an excellent compilation of the concepts that would help in understanding this article.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b068d12",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "Introduction\n",
    "Loss functions for regression\n",
    "Loss functions for classification\n",
    "Conclusion\n",
    "Further reading\n",
    "\n",
    "# 0. Introduction\n",
    "A loss function takes a theoretical proposition to a practical one. Building a highly accurate predictor requires constant iteration of the problem through questioning, modeling the problem with the chosen approach and testing.\n",
    "\n",
    "The only criteria by which a statistical model is scrutinized is its performance - how accurate the model’s decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
    "\n",
    "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
    "\n",
    "# 1. Loss functions for regression\n",
    "Regression involves predicting a specific value that is continuous in nature. Estimating the price of a house or predicting stock prices are examples of regression because one works towards building a model that would predict a real-valued quantity.\n",
    "\n",
    "Let’s take a look at some loss functions which can be used for regression problems and try to draw comparisons among them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fea27c",
   "metadata": {},
   "source": [
    "## 1.1 L1范数损失 , L1-LossMean, Absolute Error (MAE)\n",
    "\n",
    "Mean Absolute Error (also called L1 loss) is one of the most simple yet robust loss functions used for regression models.\n",
    "\n",
    "Regression problems may have variables that are not strictly Gaussian in nature due to the presence of outliers (values that are very different from the rest of the data). Mean Absolute Error would be an ideal option in such cases because it does not take into account the direction of the outliers (unrealistically high positive or negative values).\n",
    "\n",
    "As the name suggests, MAE takes the average sum of the absolute differences between the actual and the predicted values. For a data point xi and its predicted value yi, n being the total number of data points in the dataset, the mean absolute error is defined as:\n",
    "\n",
    "$$MAE = \\frac{1}{N}\\sum\\limits_{i=1}\\limits^{N-1} |y_i - x_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82679c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[-0.46473032  0.54652299  0.42551151 -1.34267024  1.44770848  0.28243456\n",
      "  0.9549279  -0.67330887  0.87979148 -0.60954231]\n",
      "1.1454870444295682 1.1454870444295682\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def my_loss_l1(x,y):\n",
    "    return np.mean(np.abs(x - y))\n",
    "\n",
    "N = 10\n",
    "x = np.random.randn(N)\n",
    "y = np.random.randn(N)\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n",
    "my_mae = my_loss_l1(x,y)\n",
    "sklearn_mae = mean_absolute_error(x, y)\n",
    "print(my_mae, sklearn_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a868014",
   "metadata": {},
   "source": [
    "# 1.2 L2范数损失, L2 loss, Mean Squared Error (MSE)\n",
    "\n",
    "Mean Squared Error (also called L2 loss) is almost every data scientist’s preference when it comes to loss functions for regression. This is because most variables can be modeled into a Gaussian distribution.\n",
    "\n",
    "Mean Squared Error is the average of the squared differences between the actual and the predicted values. For a data point Yi and its predicted value Ŷi, where n is the total number of data points in the dataset, the mean squared error is defined as:\n",
    "$$MSE = \\frac{1}{N}\\sum\\limits_{i=1}\\limits^{N-1} |y_i - x_i|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9eb6114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[-1.02390269  0.78449111  0.12764909  0.60077756 -1.38303544 -1.78699437\n",
      " -0.8337004  -0.19166717 -0.02222612  0.55830641]\n",
      "1.635810818506421 1.635810818506421\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def my_loss_l2(x,y):\n",
    "    return np.mean(np.square(x - y))\n",
    "\n",
    "N = 10\n",
    "#x = np.arange(N)\n",
    "#y = np.zeros([N,])\n",
    "x = np.random.randn(N)\n",
    "y = np.random.randn(N)\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n",
    "my_mae = my_loss_l2(x,y)\n",
    "sklearn_mae = mean_squared_error(x, y)\n",
    "print(my_mae, sklearn_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e64b8c",
   "metadata": {},
   "source": [
    "## 1.3 Mean Bias Error (MBE)\n",
    "\n",
    "Mean Bias Error is used to calculate the average bias in the model. Bias, in a nutshell, is overestimating or underestimating a parameter. Corrective measures can be taken to reduce the bias post-evaluating the model using MBE.\n",
    "\n",
    "Mean Bias Error takes the actual difference between the target and the predicted value, and not the absolute difference. One has to be cautious as the positive and the negative errors could cancel each other out, which is why it is one of the lesser-used loss functions.\n",
    "\n",
    "The formula of Mean Bias Error is:\n",
    "\n",
    "$$MBE = \\frac{1}{N}\\sum\\limits_{i=1}\\limits^{N-1} (y_i - x_i)$$\n",
    "\n",
    "这个其实很难说是一种（有用的）损失函数，scikit-learn中没有定义这种损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34040bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.009731471835487447\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_mbe(x,y):\n",
    "    return np.mean(x - y)\n",
    "\n",
    "N = 10000\n",
    "#x = np.arange(N)\n",
    "#y = np.zeros([N,])\n",
    "x = np.random.randn(N)\n",
    "y = np.random.randn(N)\n",
    "\n",
    "my_mbe = my_mbe(x,y)\n",
    "print(my_mbe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c84627",
   "metadata": {},
   "source": [
    "## 1.4 Mean Squared Logarithmic Error (MSLE)\n",
    "\n",
    "Sometimes, one may not want to penalize the model too much for predicting unscaled quantities directly. Relaxing the penalty on huge differences can be done with the help of Mean Squared Logarithmic Error.\n",
    "\n",
    "Calculating the Mean Squared Logarithmic Error is the same as Mean Squared Error, except the natural logarithm of the predicted values is used rather than the actual values.\n",
    "\n",
    "$$MSE = \\frac{1}{N}\\sum\\limits_{i=1}\\limits^{N-1} |y_i - x_i|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61697241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
