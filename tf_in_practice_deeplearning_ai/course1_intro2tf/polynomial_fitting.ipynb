{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIAkIlfmCe1B"
   },
   "source": [
    "# Polynomial fitting with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fA93WUy1zzWf"
   },
   "source": [
    "Neural networks generally won't do a good job in extrapolating polynomial functions. However, if your training and testing data are from the same range, you could achieve quite nice results.\n",
    "\n",
    "Reference: https://stackoverflow.com/questions/44998910/keras-model-to-fit-polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzbtdRcZDO9B"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Let's start with our imports. Here we are importing TensorFlow and calling it tf for ease of use.\n",
    "\n",
    "We then import a library called numpy, which helps us to represent our data as lists easily and quickly.\n",
    "\n",
    "The framework for defining a neural network as a set of Sequential layers is called keras, so we import that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9uIpOS2zx7k"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.random.rand(10000)\n",
    "y_train=x_train**4+x_train**3-x_train\n",
    "x_train=x_train.reshape(len(x_train),1)\n",
    "\n",
    "x_test=np.linspace(0,1,100)\n",
    "y_test=x_test**4+x_test**3-x_test\n",
    "x_test=x_test.reshape(len(x_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: because x_train is generated in random way, and not in order. Hence scatter is used instead of plot.\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwJGmDrQ0EoB"
   },
   "source": [
    "## Define and Compile the Neural Network\n",
    "\n",
    "Note that I increased epochs to 40 to get more iterations and more accurate results. I also set verbose=1 to be able to see how the loss behaves. The loss is indeed decreasing down to 7.4564e-04, and below is the result that I got. The red line is the prediction of the network, and the blue line is the correct value. You can see that they are quite close to each other.\n",
    "\n",
    "How about change 'relu' to other activation function, for example, sigmoid? In this example, sigmoid is by far wrose than relu.  \n",
    "In a single layer in 1d Relus are performing piecewise linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense: Units\n",
    "    \n",
    "The units are the most basic parameter to understand. This parameter is a positive integer that denotes the output size of the layer. Itâ€™s the most important parameter we can set for this layer. The unit parameter actually dictates the size of the weight matrix and bias vector (the bias vector will be the same size, but the weight matrix will be calculated based on the size of the input data so that the dot product will produce data that is of output size, units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQFAr_xo0M4T"
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "# Below is equivalent.\n",
    "#layer_0 = keras.layers.Dense(units=1, input_shape=[1])\n",
    "#model = tf.keras.Sequential([layer_0])\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=200, input_dim=1))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "#model.add(keras.layers.Activation('sigmoid'))\n",
    "model.add(keras.layers.Dense(units=45))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "#model.add(keras.layers.Activation('sigmoid'))\n",
    "model.add(keras.layers.Dense(units=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "model.fit(x_train, y_train, epochs=40, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=100, verbose=0)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(x_test, batch_size=1)\n",
    "\n",
    "test=x_test.reshape(-1)\n",
    "plt.plot(test,classes,c='r')\n",
    "plt.plot(test,y_test,c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting y = a * x^2 + b * x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example1: a = 0.1; b = 0.3; c = 0.5  \n",
    "With these parameters, y has roughly the same range as x, the model behaves well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5; b = 0.3; c = 0.5\n",
    "\n",
    "xs = np.array(np.linspace(0,1,1000))\n",
    "np.random.shuffle(xs)\n",
    "\n",
    "ys  = np.array([a * xs * xs + b * xs + c]).T\n",
    "plt.scatter(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xs, ys, epochs=40, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xs, ys, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example2: a = 1; b = 3; c = 5  \n",
    "With these parameters, y has by far larger range than x, the model behavior become worse.  \n",
    "This is an example to indicate the importance of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3; b = 3; c = 5\n",
    "#ys  = np.array([a * xs[:,0] + b * xs[:,1] + c]).T\n",
    "ys  = np.array([a * xs * xs + b * xs + c]).T\n",
    "plt.scatter(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xs, ys, epochs=40, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xs, ys, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Colab1-for-deeplearn.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
